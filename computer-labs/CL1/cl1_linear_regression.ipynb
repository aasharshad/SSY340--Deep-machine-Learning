{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer lab 1 (CL1) - Linear regression\n",
    "\n",
    "The *python-crash-course* taught you general basics of python.\n",
    "Now the computer labs *CL1* and *CL2* will specifically introduce you to python for machine learning.\n",
    "The goal is to provide you with a few hands-on examples of simple regression and classification so that you can build intuition for these types of problems and to show you a standard way of developing a machine learning algorithm in python.\n",
    "\n",
    "The labs will give you experience with a few well-known python libraries (called *modules*) which will be used in the course.\n",
    "Extra important is the machine learning module called `pytorch`, but also libraries for data manipulation and visualisation, such as `pandas`, `matplotlib` and `numpy`.\n",
    "\n",
    "We strongly encourage you to play with the code and explore ways of manipulating and plotting the data, that is how you build up your intuition.\n",
    "\n",
    "For this computer lab, we'll be using the IRIS dataset. Initially, we'll only look at a subset of it, and perform linear regression on two features of a given class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Import the necessary modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these three different modules, and one of the functions from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# This last line is a special jupyter command\n",
    "# which makes matplotlib plots show up in notebooks themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2  Read the dataset from a .csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [IRIS dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) using `pandas`. The method `read_csv(<filename>)` takes a path to a csv-file and returns a `DataFrame` object containing the data found in the file.\n",
    "\n",
    "`pandas` data frames are a great way of handling simple data, where the entire dataset can be read into the computer memory all at once.\n",
    "Later, we will handle more complex data which will require us to create so called *Dataloaders*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file `iris.csv` is located in our current directory (<repo_root>/computer-labs/CL1),\n",
    "# otherwise the function would fail.\n",
    "dataset = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3  Analyze the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is comprised of morphologic data from three different species of the Iris flowers: Setosa, Virginica and Versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th> <center>Iris Setosa</center> </th>\n",
    "    <th> <center>Iris Virginica</center> </th> \n",
    "    <th> <center>Iris Versicolor</center> </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg\" alt=\"Iris Setosa\"></td>\n",
    "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9f/Iris_virginica.jpg\" alt=\"Iris Virginica\"></td>\n",
    "    <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/Blue_Flag%2C_Ottawa.jpg\" alt=\"Iris Virginica\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lenght and width of both the petals and the sepals of each flower, together with its corresponding species were measured and stored in this dataset. Sepals and petals are both parts of a flower. Sepals are the outermost part of the whorl and the petals are the innermost part.\n",
    "![](http://terpconnect.umd.edu/~petersd/666/html/iris_with_labels.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what's inside the dataset now. The attribute `shape` of `DataFrame` objects returns the dimensions of the data inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this dataset has 150 rows and 5 columns. It's easy to infer that this means 150 flowers were collected, and 5 different features were registered for each one. We can also take a closer look at them, using the method `head()`, which returns the first 5 rows by default (you can also pass a parameter to it, which specifies a different amount of rows to be shown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the header names for each column, together with the first rows, confirming that the species and morphologic measurements for each flower were collected. We can extract individual columns of this `DataFrame` by indexing using their names, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"sepal_length\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can check which species are present in the dataset using the `unique` method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[\"species\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we see that only these three species are present in this dataset, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also learn more about the data types of each column with the method `info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the first four columns' elements are floating point numbers, and the last column's elements are objects (in this case, strings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4  Extract the desired data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this initial task, we are only interested in the setosa species. This corresponds to all the rows which have the column 'species' equal to the string 'setosa'. In order to extract these rows, we use [logical indexing in Pandas](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns a boolean series, which for every row in the dataframe\n",
    "# if a row is a setosa row or not.\n",
    "# We can use it to index our DataFrame object\n",
    "extract_rule = (dataset['species']=='setosa')\n",
    "print(extract_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the boolean series to index the DataFrame object.\n",
    "# This will give us a new dataframe which only contains the setosa rows\n",
    "setosa_dataset = dataset[extract_rule]\n",
    "setosa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract columns from the `DataFrame`.\n",
    "Suppose that we want to investigate the relationship between two features of this species, the 'sepal_length' and 'sepal_width'. To extract these, we [index the `DataFrame` using the name of the columns](https://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-label)  we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = setosa_dataset['sepal_length'].values\n",
    "y = setosa_dataset['sepal_width'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the attribute `values` in a `DataFrame` object returns a numpy array.\n",
    "That is, extracting a single column in a `DataFrame` gives us a numpy array, not another `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use matplotlib to plot all the examples in a 2D plane, where each dimension is one of the features described earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x,y)\n",
    "ax.set_xlabel('sepal length')\n",
    "ax.set_ylabel('sepal width');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the relation between these features could be approximated using a linear function, such as \n",
    "$f(x) = w\\cdot x + b$. Let's try finding the parameters $w$ and $b$ that would make the best approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5  Guess the values of w and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with some educated guesses. To make this more convenient, we'll first define a function to plot a scatter plot of the provided data, together with a straight line with parameters specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the data and a parameterized line\n",
    "def plot_data_and_line(w, b, x, y, ax, line_color='r', line_label=''):\n",
    "    \n",
    "    # Create points lying on the line\n",
    "    xline = np.unique(x)\n",
    "    yline = w*xline + b\n",
    "\n",
    "    # Plot both the line and the points from the dataset\n",
    "    ax.scatter(x,y, color='C0')\n",
    "    ax.plot(xline, yline, color=line_color, label=line_label)\n",
    "    ax.set_xlabel('sepal length')\n",
    "    ax.set_ylabel('sepal width') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_data_and_line(1, -1, x, y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, another way of evaluating the quality of our approximation is to compute the MSE ([mean squared error](https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/)) between the true y features in the dataset and our predictions. So that we can use this value as well to guide our guesses, create a function to compute it (first, it might be beneficial to write down the analytical expression for it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "84a068549164d7d4c62f18c3201a545b",
     "grade": true,
     "grade_id": "cell-17bd84c29b5dc802",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a function called `mse` to compute the mean squared error\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try different values of $w$ and $b$ and see how the resulting linear approximation looks like, compared to the scatter plot of our data. Using both the plot and the MSE, try searching for values of $w$ and $b$ that yield a good approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc0d1c7d175ebd319359d29a0a453261",
     "grade": true,
     "grade_id": "cell-5df4a2b30a9d05e5",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Here we just check that you actually implemented the `mse` function. \n",
    "try:\n",
    "    mse\n",
    "except NameError:\n",
    "    raise NotImplementedError(\n",
    "        \"You need to implement a function `mse` in the cell above. Don't forget to run the cell!\")\n",
    "\n",
    "# Guess the values for w and b\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Plot your guess\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots()\n",
    "plot_data_and_line(w, b, x, y, ax);\n",
    "\n",
    "# Compute guess\n",
    "\n",
    "y_guess = w*x+b\n",
    "\n",
    "# Some numpy operations can change the dimensions of the output.\n",
    "# e.g. it changes an array from shape (50, 1) to (50,)\n",
    "# to us this means the same thing, it's an object with 50 elements.\n",
    "# However, it can mess with later computations. To be sure we can reshape `y_guess`\n",
    "# to force it to be of the same shape as `y`.\n",
    "# (w*x+b should not do it but we'll use it as an example anyway.)\n",
    "y_guess = y_guess.reshape(y.shape)\n",
    "\n",
    "# Compute and print the MSE of the guess\n",
    "print(\"MSE of your guess:\", mse(y,y_guess))\n",
    "\n",
    "# Sanity check: the MSE for the guess (w, b) = (1, -1) should be around 0.41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Training a model for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of trying to find the parameters that give the best approximation by trial and error, we'll use `pytorch` to build and optimize a linear regressor neural network.\n",
    "\n",
    "`pytorch` is a widely used library or *module* for doing machine learning in python.\n",
    "It allows user to construct, train and evaluate general neural networks.\n",
    "A more thorough introduction of `pytorch` comes later in the labs.\n",
    "\n",
    "It is not necessary for completing these labs, but if you are interested, take a look at `pytorch`'s\n",
    "- [documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [tutorials](https://pytorch.org/tutorials/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create dataset and data loaders from the data\n",
    "\n",
    "`pytorch` expects to get data from a so called *data loader*.\n",
    "A data loader is an object that provides your machine learning model with *batches* of data.\n",
    "That way, we don't have to load our entire dataset into memory at once. Instead we get smaller, more manageable batches of data to work with.\n",
    "To create a `DataLoader` object, we first create a `TensorDataset`. The actual dataset can be stored in memory, on the hard drive or even on some remote server. `TensorDataset` is an abstraction over our data which provides methods for how to obtain a sample from our data.\n",
    "Finally, the `DataLoader` class takes an instance of our `TensorDataset` class (and some other configuration parameters) and automatically provides methods for iterating over the full data in batches.\n",
    "\n",
    "It is a bit overkill to do this with our simple Iris data (we have already seen how to do it with a `DataFrame`),\n",
    "but later in the course we will use more complex data which do require a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current shape of `x` (and `y`) is (50,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of x: {}\".format(x.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to be careful with the actual dimension of our dataset.\n",
    "\n",
    "For the class `TensorDataset` to work properly, we need to add a second dimension to `x`.\n",
    "The loader expects data of size `(N,D)`, where `N` is the number of samples in the dataset, and `D` is the dimension of each sample.\n",
    "So for our example we have:\n",
    "\n",
    "N = 50, D = 1\n",
    "\n",
    "The dimension of `y` can be `(50,)` because we won't actually pass it through the model.\n",
    "We might however need to later check so that this shape works with our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single dimension can be added in multiple ways.\n",
    "# It is good practice to use the `.reshape(<new_shape>)` function\n",
    "# since it clearly communicates what we want the new shape to be.\n",
    "\n",
    "N = x.shape[0] # x.shape is a tuple (N, ) where the first (0th) element is N\n",
    "x = x.reshape((N, 1))\n",
    "print(\"The new shape of x is {}\".format(x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataset and the data loader.\n",
    "\n",
    "Note that in the data loader we specify the batch size to match the size of the dataset.\n",
    "Typically, we will use batch sizes which are much smaller than the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
    "data_loader = DataLoader(dataset, batch_size=len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create the model for linear regression.\n",
    "\n",
    "Now we actually create our model.\n",
    "\n",
    "We define a class `LinearRegressor` which inherits from `nn.Module`.\n",
    "`nn.Module` is `pytorch`'s basic neural network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"The __init__ function creates our model\n",
    "        by creating a network which only has a single linear layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Every `pytorch` model has a forward method.\n",
    "        It describes how a network, or even a single layer transforms an input `x` to an output\n",
    "        \n",
    "        Here, we simple refer the input to our linear layer.\n",
    "        \"\"\"\n",
    "        return self.lin(x)\n",
    "\n",
    "# Create an instance of the model class.\n",
    "model = LinearRegressor()\n",
    "\n",
    "# `model` is a simple linear transformation. By default its parameters (w and b) are intialised randomly,\n",
    "# but we can still see how it works.\n",
    "\n",
    "# Let's create some made-up input data.\n",
    "# We need to jump through some hoops to make sure that the input is\n",
    "# - of type torch.Tensor\n",
    "# - with shape (1, 1)\n",
    "test_x = torch.tensor(2.4).reshape((1, 1))\n",
    "\n",
    "# Now we can use our model to predict y.\n",
    "# Note: model(x) is just a shortcut for model.forward(x).\n",
    "test_y = model(test_x)\n",
    "\n",
    "# y will change with every run, since the model parameters are randomly chosen.\n",
    "print(\"model({:.2f}) = {:.2f}\".format(test_x.item(), test_y.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the mean squared error loss.\n",
    "Although we have already implemented one above, we use the one built in to `pytorch`,\n",
    "so that we can train our model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For optimization, we use stochastic gradient descent (since our batch size is the size of the dataset, we're actually doing gradient descent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Perform the optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform the optimization by simply following these steps in a loop:\n",
    "1. Sample a batch of data from our dataset\n",
    "2. Compute the model's prediction on the batch\n",
    "3. Compute the loss of the prediction w.r.t. ground-truth\n",
    "4. Backpropagate the loss through the model's parameters\n",
    "5. Perform one step of gradient descent.\n",
    "\n",
    "We will do this for 20 epochs. Again, since our batch size is the same size of the dataset, this means we'll take 20 steps of gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    losses_in_epoch = []\n",
    "    for batch in data_loader:\n",
    "\n",
    "        # 1. These are the sampled batches of inputs and ground-truth\n",
    "        batch_x, batch_y = batch\n",
    "        \n",
    "        # 2. Compute the model's prediction on the batch\n",
    "        pred = model(batch_x)\n",
    "        \n",
    "        # 3. Compute the loss of the prediction w.r.t. ground-truth\n",
    "        loss = loss_fn(pred.squeeze(), batch_y)\n",
    "        \n",
    "        # Save losses in a list for averaging later (not sctrictly necessary for batch_size = len(x))\n",
    "        losses_in_epoch.append(loss)\n",
    "        \n",
    "        # 4. Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. One step of gradient descent\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero the gradients computed in the backpropagation, for starting new optimization step\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print('Epoch: {}\\tLoss: {}'.format(epoch, sum(losses_in_epoch)/len(losses_in_epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the final MSE obtained (~0.06). Compare it to the one obtained using the guessed parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5  Extract parameters w and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the parameters found by the optimization by using the `parameters` method of the created model (this returns a [generator](https://www.programiz.com/python-programming/generator), so we transform it into a list first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in this list is a `Parameter` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the underlying tensor by using the `data` attribute of the `Parameter` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the float inside the tensor with the `item` method (only works with one-element tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters[0].data.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this together we can get the parameters of our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star, b_star = [p.data.item() for p in parameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which results in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"w*: {:.3f}\".format(w_star))\n",
    "print(\"b*: {:.3f}\".format(b_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these optimized parameters with the ones you guessed before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5  Compare optimal and guessed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's also beneficial to compare the guessed parameters with the optimized ones graphically, by showing both of the predicted lines in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax = plt.subplots()\n",
    "plot_data_and_line(w, b, x, y, ax, 'r', 'guess')\n",
    "#plot_data_and_line(w_star, b_star, x, y, ax, 'b', 'optimal')\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
